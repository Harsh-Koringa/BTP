{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a8397d-9da3-48b4-ab9b-182a90294b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdaaccd-994a-4ee8-962c-1244538f587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture diagram generated as 'enhanced_ltc_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='EnhancedLTC Model Architecture', format='png')\n",
    "\n",
    "# Input\n",
    "dot.node('I', 'Input\\n(batch, seq_len, 22)')\n",
    "\n",
    "# LTC Layers\n",
    "dot.node('L1', 'LTC_Cell Layer 1\\n(hidden=64)')\n",
    "dot.node('L2', 'LTC_Cell Layer 2\\n(hidden=64)')\n",
    "\n",
    "# Attention\n",
    "dot.node('A', 'Attention\\n(Self-Attention)')\n",
    "\n",
    "# Classifier\n",
    "dot.node('C1', 'Linear\\n(64→32)')\n",
    "dot.node('C2', 'ReLU')\n",
    "dot.node('C3', 'Dropout(0.5)')\n",
    "dot.node('C4', 'Linear\\n(32→4)')\n",
    "\n",
    "# Output\n",
    "dot.node('O', 'Output\\n(batch, 4)')\n",
    "\n",
    "# Draw edges\n",
    "dot.edge('I', 'L1')\n",
    "dot.edge('L1', 'L2')\n",
    "dot.edge('L2', 'A')\n",
    "dot.edge('A', 'C1')\n",
    "dot.edge('C1', 'C2')\n",
    "dot.edge('C2', 'C3')\n",
    "dot.edge('C3', 'C4')\n",
    "dot.edge('C4', 'O')\n",
    "\n",
    "# Render diagram to file\n",
    "dot.render('enhanced_ltc_architecture', view=True)\n",
    "\n",
    "print(\"Architecture diagram generated as 'enhanced_ltc_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f5f395-64c7-4845-ab18-3512d3f5e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture diagram generated as 'ltc_transformer_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='LTC_Transformer Model Architecture', format='png')\n",
    "\n",
    "# Main components\n",
    "dot.node('I', 'Input\\n(batch, seq_len, 22)')\n",
    "dot.node('LTC', 'LTC Layer(s)\\n(hidden_dim=64)')\n",
    "dot.node('TF', 'Transformer Encoder\\n(layers=1, heads=2)')\n",
    "dot.node('P', 'Global Average Pooling')\n",
    "dot.node('C1', 'Linear\\n(64→32)')\n",
    "dot.node('C2', 'ReLU')\n",
    "dot.node('C3', 'Dropout(0.5)')\n",
    "dot.node('C4', 'Linear\\n(32→4)')\n",
    "dot.node('O', 'Output\\n(batch, 4)')\n",
    "\n",
    "# Draw main flow\n",
    "dot.edge('I', 'LTC')\n",
    "dot.edge('LTC', 'TF')\n",
    "dot.edge('TF', 'P')\n",
    "dot.edge('P', 'C1')\n",
    "dot.edge('C1', 'C2')\n",
    "dot.edge('C2', 'C3')\n",
    "dot.edge('C3', 'C4')\n",
    "dot.edge('C4', 'O')\n",
    "\n",
    "# Add recurrent connection for LTC\n",
    "dot.edge('LTC', 'LTC', label='Recurrent', constraint='false')\n",
    "\n",
    "# Render diagram to file\n",
    "dot.render('ltc_transformer_architecture', view=True)\n",
    "\n",
    "print(\"Architecture diagram generated as 'ltc_transformer_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6689df5-6b7e-4ac7-bc19-bb36d13d2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed architecture diagram generated as 'ltc_transformer_detailed.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='LTC_Transformer Detailed Architecture', format='png')\n",
    "\n",
    "# Input processing\n",
    "dot.node('I', 'Input\\n(batch, seq_len, 22)')\n",
    "\n",
    "# LTC processing flow\n",
    "dot.node('L_in', 'Time-step Processing')\n",
    "dot.node('L_proc', 'LTC Cell\\n(Adaptive Time Constant)')\n",
    "dot.node('L_stack', 'Hidden State Stack\\n(batch, seq_len, 64)')\n",
    "\n",
    "# Transformer processing\n",
    "dot.node('T_attn', 'Self-Attention\\n(heads=2)')\n",
    "dot.node('T_ff', 'Feed-Forward\\n(dim=128)')\n",
    "dot.node('T_out', 'Contextualized Features')\n",
    "\n",
    "# Classification\n",
    "dot.node('P', 'Global Avg Pooling\\n(batch, 64)')\n",
    "dot.node('C', 'Classifier Network\\n(64→32→4)')\n",
    "dot.node('O', 'Output\\n(batch, 4)')\n",
    "\n",
    "# Draw main flow\n",
    "dot.edge('I', 'L_in')\n",
    "dot.edge('L_in', 'L_proc')\n",
    "dot.edge('L_proc', 'L_stack')\n",
    "dot.edge('L_stack', 'T_attn')\n",
    "dot.edge('T_attn', 'T_ff')\n",
    "dot.edge('T_ff', 'T_out')\n",
    "dot.edge('T_out', 'P')\n",
    "dot.edge('P', 'C')\n",
    "dot.edge('C', 'O')\n",
    "\n",
    "# Add recurrent connections\n",
    "dot.edge('L_proc', 'L_proc', label='h(t)→h(t+1)', constraint='false')\n",
    "\n",
    "# Render diagram to file\n",
    "dot.render('ltc_transformer_detailed', view=True)\n",
    "\n",
    "print(\"Detailed architecture diagram generated as 'ltc_transformer_detailed.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a712ea77-e84c-4007-9b7a-c695995c6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture diagram generated as 'enhanced_ltc_fusion_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='EnhancedLTC Model Architecture', format='png')\n",
    "\n",
    "# Main nodes\n",
    "dot.node('Input', 'Input\\n(batch, seq_len, 22)')\n",
    "dot.node('LTC', 'LTC Layers\\n(num_layers=2, hidden_dim=64)')\n",
    "dot.node('LTC_Out', 'LTC Outputs\\n(batch, seq_len, 64)')\n",
    "\n",
    "# Attention branch\n",
    "dot.node('Attn_Calc', 'Attention\\nCalculation')\n",
    "dot.node('Attn_Weights', 'Attention\\nWeights')\n",
    "dot.node('LTC_Context', 'LTC Context\\n(batch, 64)')\n",
    "\n",
    "# LSTM branch\n",
    "dot.node('LSTM', 'LSTM Layer\\n(hidden_dim=64)')\n",
    "dot.node('LSTM_Context', 'LSTM Context\\n(batch, 64)')\n",
    "\n",
    "# Fusion and classification\n",
    "dot.node('Concat', 'Concatenate\\n(batch, 128)')\n",
    "dot.node('Fusion', 'Fusion Layer\\n(128→64)')\n",
    "dot.node('Classifier', 'Classifier\\n(64→32→4)')\n",
    "dot.node('Output', 'Output\\n(batch, 4)')\n",
    "\n",
    "# Define edges - Main flow\n",
    "dot.edge('Input', 'LTC')\n",
    "dot.edge('LTC', 'LTC_Out')\n",
    "\n",
    "# Attention branch\n",
    "dot.edge('LTC_Out', 'Attn_Calc')\n",
    "dot.edge('Attn_Calc', 'Attn_Weights')\n",
    "dot.edge('Attn_Weights', 'LTC_Context', label='weighted sum')\n",
    "dot.edge('LTC_Out', 'LTC_Context', style='dashed')\n",
    "\n",
    "# LSTM branch\n",
    "dot.edge('LTC_Out', 'LSTM')\n",
    "dot.edge('LSTM', 'LSTM_Context')\n",
    "\n",
    "# Fusion and classification\n",
    "dot.edge('LTC_Context', 'Concat')\n",
    "dot.edge('LSTM_Context', 'Concat')\n",
    "dot.edge('Concat', 'Fusion')\n",
    "dot.edge('Fusion', 'Classifier')\n",
    "dot.edge('Classifier', 'Output')\n",
    "\n",
    "# Add recurrent connection for LTC\n",
    "dot.edge('LTC', 'LTC', label='recurrent', constraint='false')\n",
    "\n",
    "# Optional: Use subgraphs to organize the diagram\n",
    "with dot.subgraph(name='cluster_attn_branch') as c:\n",
    "    c.attr(label='Attention Branch', style='dashed')\n",
    "    c.node_attr.update(style='filled', color='lightblue')\n",
    "    c.node('Attn_Calc')\n",
    "    c.node('Attn_Weights')\n",
    "    c.node('LTC_Context')\n",
    "\n",
    "with dot.subgraph(name='cluster_lstm_branch') as c:\n",
    "    c.attr(label='LSTM Branch', style='dashed')\n",
    "    c.node_attr.update(style='filled', color='lightgreen')\n",
    "    c.node('LSTM')\n",
    "    c.node('LSTM_Context')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('enhanced_ltc_fusion_architecture', view=True)\n",
    "\n",
    "print(\"Architecture diagram generated as 'enhanced_ltc_fusion_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315a9f79-8a83-4640-8235-bb826dde6195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture diagram generated as 'enhanced_ltc_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='EnhancedLTC Model Architecture', format='png')\n",
    "dot.attr(rankdir='LR')  # Add this line to change layout to left-to-right\n",
    "\n",
    "# Input\n",
    "dot.node('I', 'Input\\n(batch, seq_len, 22)')\n",
    "\n",
    "# LTC Layers\n",
    "dot.node('L1', 'LTC_Cell Layer 1\\n(hidden=64)')\n",
    "dot.node('L2', 'LTC_Cell Layer 2\\n(hidden=64)')\n",
    "\n",
    "# Attention\n",
    "dot.node('A', 'Attention\\n(Self-Attention)')\n",
    "\n",
    "# Classifier\n",
    "dot.node('C1', 'Linear\\n(64→32)')\n",
    "dot.node('C2', 'ReLU')\n",
    "dot.node('C3', 'Dropout(0.5)')\n",
    "dot.node('C4', 'Linear\\n(32→4)')\n",
    "\n",
    "# Output\n",
    "dot.node('O', 'Output\\n(batch, 4)')\n",
    "\n",
    "# Draw edges\n",
    "dot.edge('I', 'L1')\n",
    "dot.edge('L1', 'L2')\n",
    "dot.edge('L2', 'A')\n",
    "dot.edge('A', 'C1')\n",
    "dot.edge('C1', 'C2')\n",
    "dot.edge('C2', 'C3')\n",
    "dot.edge('C3', 'C4')\n",
    "dot.edge('C4', 'O')\n",
    "\n",
    "# Render diagram to file\n",
    "dot.render('enhanced_ltc_architecture', view=True)\n",
    "\n",
    "print(\"Architecture diagram generated as 'enhanced_ltc_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658509a9-e8ae-4a55-9500-acaf56522240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTC-Transformer architecture diagram generated as 'ltc_transformer_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with horizontal layout\n",
    "dot = Digraph(comment='LTC_Transformer Architecture', format='png')\n",
    "dot.attr(rankdir='LR')  # Left-to-right layout for better PPT display\n",
    "\n",
    "# Add nodes for main components\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', shape='box')\n",
    "\n",
    "# LTC processing nodes\n",
    "with dot.subgraph(name='cluster_ltc') as c:\n",
    "    c.attr(label='LTC Processing', style='rounded,filled', fillcolor='lightblue')\n",
    "    c.node('ltc_cell', 'LTC Cell\\n(Adaptive Time Constants)', shape='box')\n",
    "    c.node('ltc_out', 'LTC Hidden States\\n(batch, seq_len, 64)', shape='box')\n",
    "    \n",
    "    # Show recurrent connection\n",
    "    c.edge('ltc_cell', 'ltc_cell', label='τ', constraint='false')\n",
    "\n",
    "# Transformer nodes with explanation callouts\n",
    "with dot.subgraph(name='cluster_transformer') as c:\n",
    "    c.attr(label='Transformer Encoder', style='rounded,filled', fillcolor='lightgreen')\n",
    "    c.node('self_attn', 'Self-Attention\\n(2 heads)', shape='box')\n",
    "    c.node('feed_forward', 'Feed-Forward\\n(dim=128)', shape='box')\n",
    "    c.node('layer_norm', 'Layer Normalization', shape='box')\n",
    "    c.node('transformer_out', 'Contextualized Features', shape='box')\n",
    "    \n",
    "    # Transformer internal connections\n",
    "    c.edge('self_attn', 'layer_norm')\n",
    "    c.edge('layer_norm', 'feed_forward')\n",
    "    c.edge('feed_forward', 'transformer_out')\n",
    "\n",
    "# Add explanation nodes\n",
    "dot.node('note1', 'Self-attention allows\\ntokens to interact across\\ntime steps', shape='note', color='gray')\n",
    "dot.node('note2', 'Feed-forward processes\\neach time step\\nindependently', shape='note', color='gray')\n",
    "\n",
    "# Connect the explanation notes\n",
    "dot.edge('note1', 'self_attn', style='dashed', arrowhead='none', color='gray')\n",
    "dot.edge('note2', 'feed_forward', style='dashed', arrowhead='none', color='gray')\n",
    "\n",
    "# Dataflow connections\n",
    "dot.edge('input', 'ltc_cell')\n",
    "dot.edge('ltc_cell', 'ltc_out')\n",
    "dot.edge('ltc_out', 'self_attn')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('ltc_transformer_architecture', view=True)\n",
    "\n",
    "print(\"LTC-Transformer architecture diagram generated as 'ltc_transformer_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd1ac06-a69f-401e-93ea-ee58e8e36a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer architecture diagram generated as 'transformer_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with horizontal layout\n",
    "dot = Digraph(comment='Transformer Encoder Architecture', format='png')\n",
    "dot.attr(rankdir='LR')  # Left-to-right layout\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "\n",
    "# Create subgraph for input processing\n",
    "with dot.subgraph(name='cluster_input') as c:\n",
    "    c.attr(label='Input', style='filled', color='lightgreen', fillcolor='lightgreen')\n",
    "    c.node('input', 'EEG Sequence\\n(batch, seq_len, 22)', fillcolor='white')\n",
    "    c.node('pos_enc', 'Positional\\nEncoding', fillcolor='white')\n",
    "    c.node('embedding', 'Embedding', fillcolor='white')\n",
    "    \n",
    "    # Connect input nodes\n",
    "    c.edge('input', 'pos_enc')\n",
    "    c.edge('pos_enc', 'embedding')\n",
    "\n",
    "# Create transformer encoder blocks (multiple layers)\n",
    "with dot.subgraph(name='cluster_transformer') as c:\n",
    "    c.attr(label='Transformer Encoder', style='filled', color='lightsalmon', fillcolor='lightsalmon')\n",
    "    \n",
    "    # Layer 1\n",
    "    c.node('attn1', 'Multi-Head\\nAttention', fillcolor='darksalmon')\n",
    "    c.node('norm1', 'Add & Norm', fillcolor='white')\n",
    "    c.node('ff1', 'Feed Forward', fillcolor='white')\n",
    "    c.node('norm2', 'Add & Norm', fillcolor='white')\n",
    "    \n",
    "    # Layer 2 (additional layer)\n",
    "    c.node('attn2', 'Multi-Head\\nAttention', fillcolor='darksalmon')\n",
    "    c.node('norm3', 'Add & Norm', fillcolor='white')\n",
    "    c.node('ff2', 'Feed Forward', fillcolor='white')\n",
    "    c.node('norm4', 'Add & Norm', fillcolor='white')\n",
    "    \n",
    "    # Connect transformer nodes\n",
    "    c.edge('attn1', 'norm1')\n",
    "    c.edge('norm1', 'ff1')\n",
    "    c.edge('ff1', 'norm2')\n",
    "    c.edge('norm2', 'attn2')\n",
    "    c.edge('attn2', 'norm3')\n",
    "    c.edge('norm3', 'ff2')\n",
    "    c.edge('ff2', 'norm4')\n",
    "\n",
    "# Create output processing\n",
    "with dot.subgraph(name='cluster_output') as c:\n",
    "    c.attr(label='Output', style='filled', color='lightblue', fillcolor='lightblue')\n",
    "    c.node('pool', 'Global Pooling', fillcolor='white')\n",
    "    c.node('classifier', 'Dense Classifier', fillcolor='white')\n",
    "    c.node('output', 'Output\\n(batch, 4)', fillcolor='white')\n",
    "    \n",
    "    # Connect output nodes\n",
    "    c.edge('pool', 'classifier')\n",
    "    c.edge('classifier', 'output')\n",
    "\n",
    "# Connect the main components\n",
    "dot.edge('embedding', 'attn1')\n",
    "dot.edge('norm4', 'pool')\n",
    "\n",
    "# Add explanatory notes\n",
    "dot.node('note1', 'Learns dependencies\\nacross time steps', shape='note', color='gray', fillcolor='white')\n",
    "dot.node('note2', 'Processes each\\ntime step independently', shape='note', color='gray', fillcolor='white')\n",
    "dot.node('note3', 'Summarizes temporal\\ninformation', shape='note', color='gray', fillcolor='white')\n",
    "\n",
    "# Connect notes to components\n",
    "dot.edge('note1', 'attn1', style='dashed', color='gray', arrowhead='none')\n",
    "dot.edge('note2', 'ff1', style='dashed', color='gray', arrowhead='none')\n",
    "dot.edge('note3', 'pool', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('transformer_architecture', view=True)\n",
    "\n",
    "print(\"Transformer architecture diagram generated as 'transformer_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe53443-b6c6-424b-8434-9802f1eadda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTC-LSTM architecture diagram generated as 'ltc_lstm_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with horizontal layout\n",
    "dot = Digraph(comment='LTC_LSTM Model Architecture', format='png')\n",
    "dot.attr(rankdir='LR')  # Left-to-right layout for better PPT display\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "\n",
    "# Input processing\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', fillcolor='lightblue')\n",
    "\n",
    "# Create LTC processing subgraph\n",
    "with dot.subgraph(name='cluster_ltc') as c:\n",
    "    c.attr(label='LTC Processing', style='filled', color='lightgreen', fillcolor='lightgreen')\n",
    "    \n",
    "    # LTC cells\n",
    "    c.node('ltc1', 'LTC Cell Layer 1\\n(22→64)', fillcolor='white')\n",
    "    c.node('ltc2', 'LTC Cell Layer 2\\n(64→64)', fillcolor='white')\n",
    "    c.node('ltc_out', 'Hidden States Stack\\n(batch, seq_len, 64)', fillcolor='white')\n",
    "    \n",
    "    # Connect LTC nodes\n",
    "    c.edge('ltc1', 'ltc2')\n",
    "    c.edge('ltc2', 'ltc_out')\n",
    "    \n",
    "    # Add recurrent connections for visualization\n",
    "    c.edge('ltc1', 'ltc1', label='τ', constraint='false')\n",
    "    c.edge('ltc2', 'ltc2', label='τ', constraint='false')\n",
    "\n",
    "# Create LSTM processing subgraph\n",
    "with dot.subgraph(name='cluster_lstm') as c:\n",
    "    c.attr(label='LSTM Processing', style='filled', color='lightsalmon', fillcolor='lightsalmon')\n",
    "    c.node('lstm', 'LSTM Layer\\n(hidden_dim=64)', fillcolor='white')\n",
    "    c.node('lstm_out', 'Final Hidden State\\n(batch, 64)', fillcolor='white')\n",
    "    \n",
    "    # Connect LSTM nodes\n",
    "    c.edge('lstm', 'lstm_out')\n",
    "\n",
    "# Create classifier subgraph\n",
    "with dot.subgraph(name='cluster_classifier') as c:\n",
    "    c.attr(label='Classifier', style='filled', color='lightblue', fillcolor='lightblue')\n",
    "    c.node('fc1', 'Linear\\n(64→32)', fillcolor='white')\n",
    "    c.node('relu', 'ReLU', fillcolor='white')\n",
    "    c.node('dropout', 'Dropout(0.5)', fillcolor='white')\n",
    "    c.node('fc2', 'Linear\\n(32→4)', fillcolor='white')\n",
    "    c.node('output', 'Output\\n(batch, 4)', fillcolor='white')\n",
    "    \n",
    "    # Connect classifier nodes\n",
    "    c.edge('fc1', 'relu')\n",
    "    c.edge('relu', 'dropout')\n",
    "    c.edge('dropout', 'fc2')\n",
    "    c.edge('fc2', 'output')\n",
    "\n",
    "# Connect the main components\n",
    "dot.edge('input', 'ltc1')\n",
    "dot.edge('ltc_out', 'lstm')\n",
    "dot.edge('lstm_out', 'fc1')\n",
    "\n",
    "# Add explanatory notes\n",
    "dot.node('note1', 'Adaptive time constants\\nfor temporal dynamics', shape='note', color='gray', fillcolor='white')\n",
    "dot.node('note2', 'Processes the entire\\nsequence of LTC outputs', shape='note', color='gray', fillcolor='white')\n",
    "dot.node('note3', 'Final classification\\nwith regularization', shape='note', color='gray', fillcolor='white')\n",
    "\n",
    "# Connect notes to relevant components\n",
    "dot.edge('note1', 'ltc1', style='dashed', color='gray', arrowhead='none')\n",
    "dot.edge('note2', 'lstm', style='dashed', color='gray', arrowhead='none')\n",
    "dot.edge('note3', 'dropout', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('ltc_lstm_architecture', view=True)\n",
    "\n",
    "print(\"LTC-LSTM architecture diagram generated as 'ltc_lstm_architecture.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6b2bf5-0724-46be-b7e5-2f6e1f116ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical LTC-LSTM architecture diagram generated as 'ltc_lstm_architecture_vertical.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with vertical layout\n",
    "dot = Digraph(comment='LTC_LSTM Model Architecture', format='png')\n",
    "dot.attr(rankdir='TB')  # Top-to-bottom layout (vertical)\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "dot.attr(size='7.5,10')  # Set size for 4:3 aspect ratio\n",
    "\n",
    "# Input processing\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', fillcolor='lightblue')\n",
    "\n",
    "# LTC processing - grouped with same color\n",
    "dot.node('ltc1', 'LTC Cell Layer 1\\n(22→64)', fillcolor='lightgreen')\n",
    "dot.node('ltc2', 'LTC Cell Layer 2\\n(64→64)', fillcolor='lightgreen')\n",
    "dot.node('ltc_out', 'Hidden States Stack\\n(batch, seq_len, 64)', fillcolor='lightgreen')\n",
    "\n",
    "# LSTM processing\n",
    "dot.node('lstm', 'LSTM Layer\\n(hidden_dim=64)', fillcolor='lightsalmon')\n",
    "dot.node('lstm_out', 'Final Hidden State\\n(batch, 64)', fillcolor='lightsalmon')\n",
    "\n",
    "# Classifier\n",
    "dot.node('fc1', 'Linear\\n(64→32)', fillcolor='lightblue')\n",
    "dot.node('relu', 'ReLU', fillcolor='lightblue')\n",
    "dot.node('dropout', 'Dropout(0.5)', fillcolor='lightblue')\n",
    "dot.node('fc2', 'Linear\\n(32→4)', fillcolor='lightblue')\n",
    "dot.node('output', 'Output\\n(batch, 4)', fillcolor='lightblue')\n",
    "\n",
    "# Connect all nodes in main flow\n",
    "dot.edge('input', 'ltc1')\n",
    "dot.edge('ltc1', 'ltc2')\n",
    "dot.edge('ltc2', 'ltc_out')\n",
    "dot.edge('ltc_out', 'lstm')\n",
    "dot.edge('lstm', 'lstm_out')\n",
    "dot.edge('lstm_out', 'fc1')\n",
    "dot.edge('fc1', 'relu')\n",
    "dot.edge('relu', 'dropout')\n",
    "dot.edge('dropout', 'fc2')\n",
    "dot.edge('fc2', 'output')\n",
    "\n",
    "# Add recurrent connections\n",
    "dot.edge('ltc1', 'ltc1', label='τ', constraint='false')\n",
    "dot.edge('ltc2', 'ltc2', label='τ', constraint='false')\n",
    "\n",
    "# Add explanation notes to the side\n",
    "dot.node('note1', 'Adaptive time constants', shape='note', color='gray', fillcolor='white')\n",
    "dot.edge('note1', 'ltc1', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('ltc_lstm_architecture_vertical', view=True)\n",
    "\n",
    "print(\"Vertical LTC-LSTM architecture diagram generated as 'ltc_lstm_architecture_vertical.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5c018f-5c73-4390-b3ee-8987e15a7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical LTC-Transformer architecture diagram generated as 'ltc_transformer_vertical.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with vertical layout\n",
    "dot = Digraph(comment='LTC_Transformer Architecture', format='png')\n",
    "dot.attr(rankdir='TB')  # Top-to-bottom layout (vertical)\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "dot.attr(size='7.5,10')  # Set size for 4:3 aspect ratio\n",
    "\n",
    "# Input\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', fillcolor='lightblue')\n",
    "\n",
    "# LTC Processing (in green like the embedding/tokenization)\n",
    "dot.node('ltc', 'LTC Cell Layer\\n(input_dim→64)', fillcolor='lightgreen')\n",
    "dot.node('ltc_out', 'LTC Hidden States\\n(batch, seq_len, 64)', fillcolor='lightgreen')\n",
    "\n",
    "# Add recurrent connection\n",
    "dot.edge('ltc', 'ltc', label='τ', constraint='false')\n",
    "\n",
    "# Transformer section (in orange like the reference image)\n",
    "dot.node('attn1', 'Attention\\n(heads=2)', fillcolor='darkorange', style='filled')\n",
    "dot.node('ff1', 'Feedforward', fillcolor='lightsalmon')\n",
    "\n",
    "# Transformer container box (just visual grouping)\n",
    "with dot.subgraph(name='cluster_transformer') as c:\n",
    "    c.attr(label='Transformer Encoder', style='filled,rounded', fillcolor='lightsalmon')\n",
    "    c.node_attr['style'] = 'filled'\n",
    "    c.node('attn1_placeholder', '', style='invis')\n",
    "    c.node('ff1_placeholder', '', style='invis')\n",
    "\n",
    "# Pooling and classification (output section in light blue)\n",
    "dot.node('pool', 'Global Average Pooling', fillcolor='lightblue')\n",
    "dot.node('fc1', 'Linear (64→32)\\nReLU\\nDropout(0.5)', fillcolor='lightblue')\n",
    "dot.node('fc2', 'Linear (32→4)', fillcolor='lightblue')\n",
    "dot.node('output', 'Output\\n(batch, 4)', fillcolor='lightblue')\n",
    "\n",
    "# Connect the main flow\n",
    "dot.edge('input', 'ltc')\n",
    "dot.edge('ltc', 'ltc_out')\n",
    "dot.edge('ltc_out', 'attn1')\n",
    "dot.edge('attn1', 'ff1')\n",
    "dot.edge('ff1', 'pool')\n",
    "dot.edge('pool', 'fc1')\n",
    "dot.edge('fc1', 'fc2')\n",
    "dot.edge('fc2', 'output')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('ltc_transformer_vertical', view=True)\n",
    "\n",
    "print(\"Vertical LTC-Transformer architecture diagram generated as 'ltc_transformer_vertical.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653f1b9f-4e1a-4b77-b16d-2f0360e0c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical LTC-Transformer architecture diagram generated as 'ltc_transformer_architecture_vertical.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with vertical layout for LTC_Transformer\n",
    "dot = Digraph(comment='LTC_Transformer Model Architecture', format='png')\n",
    "dot.attr(rankdir='TB')  # Top-to-bottom layout (vertical)\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "dot.attr(size='7.5,10')  # Set size for 4:3 aspect ratio\n",
    "\n",
    "# Input processing\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', fillcolor='lightblue')\n",
    "\n",
    "# LTC processing - grouped with same color\n",
    "dot.node('ltc', 'LTC Cell Layers\\n(22→64)', fillcolor='lightgreen')\n",
    "\n",
    "# Transformer processing\n",
    "dot.node('transformer', 'Transformer Encoder\\n(hidden_dim=64, nhead=2)', fillcolor='lightsalmon')\n",
    "dot.node('transformer_out', 'Transformer Output\\n(batch, seq_len, 64)', fillcolor='lightsalmon')\n",
    "\n",
    "# Classifier\n",
    "dot.node('fc1', 'Linear\\n(64→32)', fillcolor='lightblue')\n",
    "dot.node('relu', 'ReLU', fillcolor='lightblue')\n",
    "dot.node('dropout', 'Dropout(0.5)', fillcolor='lightblue')\n",
    "dot.node('fc2', 'Linear\\n(32→4)', fillcolor='lightblue')\n",
    "dot.node('output', 'Output\\n(batch, 4)', fillcolor='lightblue')\n",
    "\n",
    "# Connect all nodes in main flow\n",
    "dot.edge('input', 'ltc')\n",
    "dot.edge('ltc', 'transformer')\n",
    "dot.edge('transformer', 'transformer_out')\n",
    "dot.edge('transformer_out', 'fc1')\n",
    "dot.edge('fc1', 'relu')\n",
    "dot.edge('relu', 'dropout')\n",
    "dot.edge('dropout', 'fc2')\n",
    "dot.edge('fc2', 'output')\n",
    "\n",
    "# Add explanation notes to the side\n",
    "dot.node('note1', 'Adaptive time constants in LTC Cells', shape='note', color='gray', fillcolor='white')\n",
    "dot.edge('note1', 'ltc', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "dot.node('note2', 'Transformer with multi-head attention', shape='note', color='gray', fillcolor='white')\n",
    "dot.edge('note2', 'transformer', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('ltc_transformer_architecture_vertical', view=True)\n",
    "\n",
    "print(\"Vertical LTC-Transformer architecture diagram generated as 'ltc_transformer_architecture_vertical.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59fd93c4-9a32-49c5-a0d9-e30e45844be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical EnhancedLTC architecture diagram generated as 'enhanced_ltc_architecture_vertical.png'\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with vertical layout\n",
    "dot = Digraph(comment='EnhancedLTC Model Architecture', format='png')\n",
    "dot.attr(rankdir='TB')  # Top-to-bottom layout (vertical)\n",
    "dot.attr('node', shape='box', style='filled', fontname='Arial')\n",
    "dot.attr(size='7.5,10')  # Set size for 4:3 aspect ratio\n",
    "\n",
    "# Input processing\n",
    "dot.node('input', 'Input Sequence\\n(batch, seq_len, 22)', fillcolor='lightblue')\n",
    "\n",
    "# LTC processing layers\n",
    "dot.node('ltc1', 'LTC Cell Layer 1\\n(22→64)', fillcolor='lightgreen')\n",
    "dot.node('ltc2', 'LTC Cell Layer 2\\n(64→64)', fillcolor='lightgreen')\n",
    "dot.node('ltc_out', 'Hidden States Stack\\n(batch, seq_len, 64)', fillcolor='lightgreen')\n",
    "\n",
    "# Attention mechanism\n",
    "dot.node('attention', 'Attention Layer\\n(learned weights)', fillcolor='plum')\n",
    "dot.node('context', 'Context Vector\\n(batch, 64)', fillcolor='plum')\n",
    "\n",
    "# Classifier\n",
    "dot.node('fc1', 'Linear\\n(64→32)', fillcolor='lightblue')\n",
    "dot.node('relu', 'ReLU', fillcolor='lightblue')\n",
    "dot.node('dropout', 'Dropout(0.5)', fillcolor='lightblue')\n",
    "dot.node('fc2', 'Linear\\n(32→4)', fillcolor='lightblue')\n",
    "dot.node('output', 'Output\\n(batch, 4)', fillcolor='lightblue')\n",
    "\n",
    "# Connect all nodes in main flow\n",
    "dot.edge('input', 'ltc1')\n",
    "dot.edge('ltc1', 'ltc2')\n",
    "dot.edge('ltc2', 'ltc_out')\n",
    "dot.edge('ltc_out', 'attention')\n",
    "dot.edge('attention', 'context')\n",
    "dot.edge('context', 'fc1')\n",
    "dot.edge('fc1', 'relu')\n",
    "dot.edge('relu', 'dropout')\n",
    "dot.edge('dropout', 'fc2')\n",
    "dot.edge('fc2', 'output')\n",
    "\n",
    "# Add recurrent connections\n",
    "dot.edge('ltc1', 'ltc1', label='τ', constraint='false')\n",
    "dot.edge('ltc2', 'ltc2', label='τ', constraint='false')\n",
    "\n",
    "# Add explanation notes\n",
    "dot.node('note1', 'Adaptive time constants', shape='note', color='gray', fillcolor='white')\n",
    "dot.edge('note1', 'ltc1', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "dot.node('note2', 'Learned attention weights\\nover sequence', shape='note', color='gray', fillcolor='white')\n",
    "dot.edge('note2', 'attention', style='dashed', color='gray', arrowhead='none')\n",
    "\n",
    "# Render the diagram\n",
    "dot.render('enhanced_ltc_architecture_vertical', view=True)\n",
    "\n",
    "print(\"Vertical EnhancedLTC architecture diagram generated as 'enhanced_ltc_architecture_vertical.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f1713-2702-44ee-82cb-690f760eaa29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
