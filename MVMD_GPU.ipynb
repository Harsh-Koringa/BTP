{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01152a5d-f6fa-4706-84a6-70d7b44dfa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trial: (4000, 306)\n",
      "Processing trial 1, shape: (306, 4000)\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.98 GiB. GPU 0 has a total capacity of 4.00 GiB of which 3.17 GiB is free. Of the allocated memory 23.38 MiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m trial_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trial_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Apply MVMD\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m u, u_hat, omega \u001b[38;5;241m=\u001b[39m \u001b[43mMVMD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# u shape: (K, C, T) â†’ we want to store as (K, T, C)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m u_all[i] \u001b[38;5;241m=\u001b[39m u      \u001b[38;5;66;03m# (K, T, C)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mMVMD\u001b[1;34m(signal, alpha, tau, K, DC, init, tol)\u001b[0m\n\u001b[0;32m     24\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     25\u001b[0m Alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(K, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 26\u001b[0m u_hat_plus \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfreqs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m omega_plus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, K), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.98 GiB. GPU 0 has a total capacity of 4.00 GiB of which 3.17 GiB is free. Of the allocated memory 23.38 MiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# === MVMD parameters ===\n",
    "alpha = 2000\n",
    "tau = 0\n",
    "K = 4\n",
    "DC = True\n",
    "init = 1\n",
    "tol = 1e-6\n",
    "\n",
    "# === Initialize storage for MVMD results ===\n",
    "num_trials = 3\n",
    "timepoints = 4000  # from index 2001 to 6001\n",
    "channels = 306\n",
    "\n",
    "u_all = np.zeros((num_trials, K, timepoints, channels))\n",
    "u_hat_all = np.zeros((num_trials, K, timepoints, channels), dtype=np.complex64)\n",
    "omega_all = np.zeros((num_trials, K, 2))\n",
    "\n",
    "# === Load and process trials ===\n",
    "file_path = r\"D:\\BTP\\sub-1_ses-1_task-bcimici_meg.mat\"\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    data = f['dataMAT']\n",
    "    #trial_refs = data['trial'][0]\n",
    "    trial_refs = data['trial']\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial_ref = trial_refs[i,0]\n",
    "        trial_data = f[trial_ref][2001:6001]  # (channels, timepoints)\n",
    "        #first_trial = f[first_trial_ref][2001:6001]\n",
    "        print(\"Shape of trial:\", trial_data.shape)\n",
    "        # Transpose to (channels, timepoints) if needed\n",
    "        if trial_data.shape[0] != channels:\n",
    "            trial_data = trial_data.T\n",
    "\n",
    "        print(f\"Processing trial {i+1}, shape: {trial_data.shape}\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Running on device: {device}\")\n",
    "        trial_tensor = torch.tensor(trial_data, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "        # Apply MVMD\n",
    "        u, u_hat, omega = MVMD(trial_tensor, alpha, tau, K, DC, init, tol)\n",
    "        # u shape: (K, C, T) â†’ we want to store as (K, T, C)\n",
    "        u_all[i] = u      # (K, T, C)\n",
    "        u_hat_all[i] = u_hat\n",
    "        omega_all[i] = np.transpose(omega,(1,0))                        # (K, C)\n",
    "\n",
    "# === Save everything into one .mat file ===\n",
    "save_dict = {\n",
    "    'u': u_all,            # shape (3, 4, 4000, 306)\n",
    "    'u_hat': u_hat_all,    # shape (3, 4, 4000, 306)\n",
    "    'omega': omega_all     # shape (3, 4, 306)\n",
    "}\n",
    "\n",
    "savemat('mvmd_3_trials_N10_K04_02.mat', save_dict)\n",
    "print(\"Saved to mvmd_3_trials_N10_K04.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18d6b5c-545d-4850-be25-b5787d2f26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.fft import fft, ifft, fftshift, ifftshift\n",
    "\n",
    "def MVMD(signal, alpha, tau, K, DC, init, tol):\n",
    "    device = signal.device\n",
    "    x, y = signal.shape\n",
    "    if x > y:\n",
    "        C = y\n",
    "        signal = signal.T\n",
    "    else:\n",
    "        C = x\n",
    "\n",
    "    T = signal.shape[1]\n",
    "    t = torch.arange(1, T + 1, device=device) / T\n",
    "    freqs = t - 0.5 - 1 / T\n",
    "\n",
    "    # FFT of the signal\n",
    "    f_hat = fftshift(fft(signal, dim=1), dim=1)\n",
    "    f_hat_plus = f_hat.clone()\n",
    "    f_hat_plus[:, :T // 2] = 0  # Set the negative frequencies to zero\n",
    "\n",
    "    # Initialization\n",
    "    N = 300\n",
    "    Alpha = alpha * torch.ones(K, device=device)\n",
    "    u_hat_plus = torch.zeros((N + 1, len(freqs), K, C), dtype=torch.complex64, device=device)\n",
    "    omega_plus = torch.zeros((N + 1, K), device=device)\n",
    "\n",
    "    if init == 1:\n",
    "        omega_plus[0, :] = (0.5 / K) * torch.arange(K, device=device)\n",
    "    elif init == 2:\n",
    "        omega_plus[0, :] = torch.sort(torch.exp(torch.log(torch.tensor(1. / T, device=device)) + \n",
    "                                         (torch.log(torch.tensor(0.5, device=device)) - \n",
    "                                          torch.log(torch.tensor(1. / T, device=device))) * \n",
    "                                         torch.rand(K, device=device)))[0]\n",
    "    else:\n",
    "        omega_plus[0, :] = 0\n",
    "\n",
    "    if DC:\n",
    "        omega_plus[0, 0] = 0\n",
    "\n",
    "    lambda_hat = torch.zeros((N + 1, len(freqs), C), dtype=torch.complex64, device=device)\n",
    "    uDiff = tol + torch.finfo(torch.float32).eps\n",
    "    n = 0\n",
    "    sum_uk = torch.zeros((C, len(freqs)), dtype=torch.complex64, device=device)\n",
    "\n",
    "    # Main loop\n",
    "    while uDiff > tol and n < N:\n",
    "        k = 0\n",
    "        for c in range(C):\n",
    "            sum_uk[c, :] = u_hat_plus[n, :, K - 1, c] + sum_uk[c, :] - u_hat_plus[n, :, k, c]\n",
    "            u_hat_plus[n + 1, :, k, c] = (f_hat_plus[c, :] - sum_uk[c, :] - lambda_hat[n, :, c] / 2) / \\\n",
    "                                         (1 + Alpha[k] * (freqs - omega_plus[n, k]) ** 2)\n",
    "\n",
    "        if not DC:\n",
    "            temp1 = 0\n",
    "            temp2 = 0\n",
    "            for c in range(C):\n",
    "                temp1 += torch.sum(freqs * torch.abs(u_hat_plus[n + 1, :, k, c]) ** 2)\n",
    "                temp2 += torch.sum(torch.abs(u_hat_plus[n + 1, :, k, c]) ** 2)\n",
    "            omega_plus[n + 1, k] = temp1 / temp2\n",
    "\n",
    "        for k in range(1, K):\n",
    "            for c in range(C):\n",
    "                sum_uk[c, :] = u_hat_plus[n + 1, :, k - 1, c] + sum_uk[c, :] - u_hat_plus[n, :, k, c]\n",
    "                u_hat_plus[n + 1, :, k, c] = (f_hat_plus[c, :] - sum_uk[c, :] - lambda_hat[n, :, c] / 2) / \\\n",
    "                                              (1 + Alpha[k] * (freqs - omega_plus[n, k]) ** 2)\n",
    "\n",
    "            temp1 = 0\n",
    "            temp2 = 0\n",
    "            for c in range(C):\n",
    "                temp1 += torch.sum(freqs * torch.abs(u_hat_plus[n + 1, :, k, c]) ** 2)\n",
    "                temp2 += torch.sum(torch.abs(u_hat_plus[n + 1, :, k, c]) ** 2)\n",
    "            omega_plus[n + 1, k] = temp1 / temp2\n",
    "\n",
    "        for c in range(C):\n",
    "            lambda_hat[n + 1, :, c] = lambda_hat[n, :, c] + tau * (\n",
    "                torch.sum(u_hat_plus[n + 1, :, :, c], dim=1) - f_hat_plus[c, :])\n",
    "\n",
    "        n += 1\n",
    "        uDiff = torch.finfo(torch.float32).eps\n",
    "        for i in range(K):\n",
    "            for c in range(C):\n",
    "                uDiff += torch.sum(torch.abs(u_hat_plus[n, :, i, c] - u_hat_plus[n - 1, :, i, c]) ** 2)\n",
    "\n",
    "    N = min(N, n)\n",
    "    omega = omega_plus[:N, :].cpu().numpy()\n",
    "\n",
    "    u_hat = u_hat_plus[N, :, :, :].permute(1, 0, 2)\n",
    "    u = torch.zeros((K, T, C), dtype=torch.float32, device=device)\n",
    "    for k in range(K):\n",
    "        for c in range(C):\n",
    "            u[k, :, c] = torch.real(ifft(ifftshift(u_hat[k, :, c], dim=0)))\n",
    "\n",
    "    return u.cpu().numpy(), u_hat.cpu().numpy(), omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a609d50-876a-42c9-bec4-255327f91d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
