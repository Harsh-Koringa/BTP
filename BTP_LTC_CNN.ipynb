{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38347714-1b68-4307-9a4a-0c4f1707941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected shape at trial 236: (727, 22)\n",
      "Signals shape: (236, 750, 22)\n",
      "Labels shape: (236,)\n",
      "Unique labels: [769 770 771 772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('A09T.npz')  \n",
    "signal = data['s']  # Assuming shape is (total_samples, 22)\n",
    "event_types = data['etyp'].T[0]\n",
    "event_positions = data['epos'].T[0]\n",
    "\n",
    "# Initialize empty arrays\n",
    "signals = []\n",
    "trial_types = []\n",
    "valid_labels = {769, 770, 771, 772}\n",
    "\n",
    "for i in range(0, len(event_positions) - 1):\n",
    "    event_type = event_types[i]\n",
    "    next_event_type = event_types[i + 1]\n",
    "    \n",
    "    if event_type == 768 and next_event_type in valid_labels:  # Valid trial start\n",
    "        pos = event_positions[i+1]\n",
    "        \n",
    "        # Extract 750 samples x 22 channels\n",
    "        trial_signal = signal[pos+750 : pos+1500, 0:22]  # All 22 channels\n",
    "        \n",
    "        # Verify the shape is correct\n",
    "        if trial_signal.shape != (750, 22):\n",
    "            print(f\"Unexpected shape at trial {len(signals)}: {trial_signal.shape}\")\n",
    "            continue\n",
    "            \n",
    "        signals.append(trial_signal)\n",
    "        trial_types.append(next_event_type)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "signals_array = np.array(signals)  # Shape (288, 750, 22)\n",
    "labels_array = np.array(trial_types)  # Shape (288,)\n",
    "\n",
    "# Verify final shapes\n",
    "print(\"Signals shape:\", signals_array.shape)\n",
    "print(\"Labels shape:\", labels_array.shape)\n",
    "print(\"Unique labels:\", np.unique(labels_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddac0f4f-c810-4fa9-9f8b-1b739e7b899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, trials, labels):\n",
    "        # Trials: (num_trials, 750, 22)\n",
    "        # Labels: (num_trials,)\n",
    "        \n",
    "        # Normalize data per channel\n",
    "        self.data = torch.tensor(trials, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Map original labels to 0-3\n",
    "        self.label_mapping = {769: 0, 770: 1, 771: 2, 772: 3}\n",
    "        self.labels = torch.tensor([self.label_mapping[int(x)] for x in labels])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Assuming you have loaded your data into trials and labels arrays\n",
    "trials = np.load('eeg_signals.npy')  # Shape (288, 750, 22)\n",
    "labels = np.load('eeg_labels.npy')     # Shape (288,)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    trials, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "test_dataset = EEGDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "458982ab-8d20-490d-96d2-633d173bcae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleHybridLTC(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(22, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (ltc_layers): ModuleList(\n",
      "    (0): LTC_Cell(\n",
      "      (W_xh): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (W_hh): Linear(in_features=64, out_features=64, bias=False)\n",
      "      (W_tau): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# LTC_Cell as provided\n",
    "class LTC_Cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LTC_Cell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_xh = nn.Linear(input_dim, hidden_dim)\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_tau = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, x, h):\n",
    "        tau = torch.sigmoid(self.W_tau(h)) + 0.1\n",
    "        dh = -h / tau + torch.tanh(self.W_xh(x) + self.W_hh(h))\n",
    "        return h + 0.1 * dh\n",
    "\n",
    "class SimpleHybridLTC(nn.Module):\n",
    "    def __init__(self, input_dim=22, cnn_dim=64, ltc_hidden_dim=64, num_classes=4, num_ltc_layers=1):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, cnn_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(cnn_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ltc_layers = nn.ModuleList([\n",
    "            LTC_Cell(cnn_dim if i==0 else ltc_hidden_dim, ltc_hidden_dim)\n",
    "            for i in range(num_ltc_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(ltc_hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq_len, cnn_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hiddens = [torch.zeros(batch_size, self.ltc_layers[0].hidden_dim, device=x.device)]\n",
    "        all_hidden = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            hiddens[0] = self.ltc_layers[0](x_t, hiddens[0])\n",
    "            all_hidden.append(hiddens[0])\n",
    "        hidden_stack = torch.stack(all_hidden, dim=1)\n",
    "        pooled = hidden_stack.mean(dim=1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# Example instantiation\n",
    "model = SimpleHybridLTC(\n",
    "    input_dim=22,         # Number of EEG channels\n",
    "    cnn_dim=32,           # CNN output channels\n",
    "    ltc_hidden_dim=64,   # LTC hidden size\n",
    "    num_classes=4,        # Number of classes\n",
    "    num_ltc_layers=1,     # Number of stacked LTC layers \n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f6fdb9a-2d36-4cb2-a51d-cf252f2cd6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_29812\\3877300072.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint.\n",
      "Epoch 1/100 | Train Loss: 1.3638 | Val Loss: 1.3774 | Val Acc: 25.00% | Same accuracy streak: 0/10\n",
      "Epoch 2/100 | Train Loss: 1.3825 | Val Loss: 1.3744 | Val Acc: 27.08% | Same accuracy streak: 0/10\n",
      "Epoch 3/100 | Train Loss: 1.3470 | Val Loss: 1.3760 | Val Acc: 22.92% | Same accuracy streak: 0/10\n",
      "Epoch 4/100 | Train Loss: 1.3515 | Val Loss: 1.3703 | Val Acc: 25.00% | Same accuracy streak: 0/10\n",
      "Epoch 5/100 | Train Loss: 1.3372 | Val Loss: 1.3760 | Val Acc: 29.17% | Same accuracy streak: 0/10\n",
      "Epoch 6/100 | Train Loss: 1.3577 | Val Loss: 1.3783 | Val Acc: 22.92% | Same accuracy streak: 0/10\n",
      "Epoch 7/100 | Train Loss: 1.3370 | Val Loss: 1.3717 | Val Acc: 29.17% | Same accuracy streak: 0/10\n",
      "Epoch 8/100 | Train Loss: 1.3395 | Val Loss: 1.3821 | Val Acc: 29.17% | Same accuracy streak: 1/10\n",
      "Epoch 9/100 | Train Loss: 1.3639 | Val Loss: 1.4493 | Val Acc: 22.92% | Same accuracy streak: 0/10\n",
      "Epoch 10/100 | Train Loss: 1.3650 | Val Loss: 1.4078 | Val Acc: 22.92% | Same accuracy streak: 1/10\n",
      "Epoch 11/100 | Train Loss: 1.3965 | Val Loss: 1.3984 | Val Acc: 25.00% | Same accuracy streak: 0/10\n",
      "Epoch 12/100 | Train Loss: 1.3981 | Val Loss: 1.3993 | Val Acc: 33.33% | Same accuracy streak: 0/10\n",
      "Epoch 13/100 | Train Loss: 1.3277 | Val Loss: 1.3935 | Val Acc: 29.17% | Same accuracy streak: 0/10\n",
      "Epoch 14/100 | Train Loss: 1.3442 | Val Loss: 1.3907 | Val Acc: 27.08% | Same accuracy streak: 0/10\n",
      "Epoch 15/100 | Train Loss: 1.3321 | Val Loss: 1.3951 | Val Acc: 25.00% | Same accuracy streak: 0/10\n",
      "Epoch 16/100 | Train Loss: 1.3399 | Val Loss: 1.4006 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 17/100 | Train Loss: 1.3409 | Val Loss: 1.4020 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 18/100 | Train Loss: 1.3248 | Val Loss: 1.4028 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 19/100 | Train Loss: 1.3236 | Val Loss: 1.4035 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 20/100 | Train Loss: 1.3293 | Val Loss: 1.4035 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 21/100 | Train Loss: 1.3382 | Val Loss: 1.4034 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 22/100 | Train Loss: 1.3277 | Val Loss: 1.4036 | Val Acc: 14.58% | Same accuracy streak: 2/10\n",
      "Epoch 23/100 | Train Loss: 1.3269 | Val Loss: 1.4039 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 24/100 | Train Loss: 1.3273 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 25/100 | Train Loss: 1.3504 | Val Loss: 1.4045 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 26/100 | Train Loss: 1.3411 | Val Loss: 1.4047 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 27/100 | Train Loss: 1.3243 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 28/100 | Train Loss: 1.3380 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 29/100 | Train Loss: 1.3110 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 3/10\n",
      "Epoch 30/100 | Train Loss: 1.3434 | Val Loss: 1.4048 | Val Acc: 16.67% | Same accuracy streak: 4/10\n",
      "Epoch 31/100 | Train Loss: 1.3222 | Val Loss: 1.4044 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 32/100 | Train Loss: 1.3281 | Val Loss: 1.4039 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 33/100 | Train Loss: 1.3172 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 2/10\n",
      "Epoch 34/100 | Train Loss: 1.3355 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 3/10\n",
      "Epoch 35/100 | Train Loss: 1.3266 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 4/10\n",
      "Epoch 36/100 | Train Loss: 1.3261 | Val Loss: 1.4043 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 37/100 | Train Loss: 1.3365 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 38/100 | Train Loss: 1.3486 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 39/100 | Train Loss: 1.3337 | Val Loss: 1.4044 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 40/100 | Train Loss: 1.3230 | Val Loss: 1.4045 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 41/100 | Train Loss: 1.3299 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 2/10\n",
      "Epoch 42/100 | Train Loss: 1.3356 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 3/10\n",
      "Epoch 43/100 | Train Loss: 1.3260 | Val Loss: 1.4039 | Val Acc: 14.58% | Same accuracy streak: 4/10\n",
      "Epoch 44/100 | Train Loss: 1.3342 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 5/10\n",
      "Epoch 45/100 | Train Loss: 1.3220 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 6/10\n",
      "Epoch 46/100 | Train Loss: 1.3296 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 47/100 | Train Loss: 1.3347 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 48/100 | Train Loss: 1.3422 | Val Loss: 1.4042 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 49/100 | Train Loss: 1.3198 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 50/100 | Train Loss: 1.3324 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 51/100 | Train Loss: 1.3227 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 2/10\n",
      "Epoch 52/100 | Train Loss: 1.3327 | Val Loss: 1.4038 | Val Acc: 14.58% | Same accuracy streak: 3/10\n",
      "Epoch 53/100 | Train Loss: 1.3303 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 4/10\n",
      "Epoch 54/100 | Train Loss: 1.3292 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 55/100 | Train Loss: 1.3139 | Val Loss: 1.4047 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 56/100 | Train Loss: 1.3239 | Val Loss: 1.4048 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 57/100 | Train Loss: 1.3152 | Val Loss: 1.4048 | Val Acc: 16.67% | Same accuracy streak: 3/10\n",
      "Epoch 58/100 | Train Loss: 1.3151 | Val Loss: 1.4043 | Val Acc: 16.67% | Same accuracy streak: 4/10\n",
      "Epoch 59/100 | Train Loss: 1.3219 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 60/100 | Train Loss: 1.3356 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 61/100 | Train Loss: 1.3177 | Val Loss: 1.4044 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 62/100 | Train Loss: 1.3357 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 63/100 | Train Loss: 1.3394 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 2/10\n",
      "Epoch 64/100 | Train Loss: 1.3226 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 3/10\n",
      "Epoch 65/100 | Train Loss: 1.3455 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 4/10\n",
      "Epoch 66/100 | Train Loss: 1.3223 | Val Loss: 1.4041 | Val Acc: 14.58% | Same accuracy streak: 5/10\n",
      "Epoch 67/100 | Train Loss: 1.3221 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 6/10\n",
      "Epoch 68/100 | Train Loss: 1.3385 | Val Loss: 1.4039 | Val Acc: 14.58% | Same accuracy streak: 7/10\n",
      "Epoch 69/100 | Train Loss: 1.3355 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 8/10\n",
      "Epoch 70/100 | Train Loss: 1.3262 | Val Loss: 1.4043 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 71/100 | Train Loss: 1.3358 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 72/100 | Train Loss: 1.3477 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 73/100 | Train Loss: 1.3395 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 74/100 | Train Loss: 1.3394 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 75/100 | Train Loss: 1.3246 | Val Loss: 1.4044 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 76/100 | Train Loss: 1.3201 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 77/100 | Train Loss: 1.3337 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 78/100 | Train Loss: 1.3319 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 79/100 | Train Loss: 1.3300 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 80/100 | Train Loss: 1.3292 | Val Loss: 1.4048 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 81/100 | Train Loss: 1.3265 | Val Loss: 1.4044 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 82/100 | Train Loss: 1.3399 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 83/100 | Train Loss: 1.3359 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 84/100 | Train Loss: 1.3339 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 85/100 | Train Loss: 1.3231 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 86/100 | Train Loss: 1.3260 | Val Loss: 1.4043 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 87/100 | Train Loss: 1.3169 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 3/10\n",
      "Epoch 88/100 | Train Loss: 1.3371 | Val Loss: 1.4044 | Val Acc: 16.67% | Same accuracy streak: 4/10\n",
      "Epoch 89/100 | Train Loss: 1.3293 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 5/10\n",
      "Epoch 90/100 | Train Loss: 1.3189 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 91/100 | Train Loss: 1.3184 | Val Loss: 1.4044 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 92/100 | Train Loss: 1.3385 | Val Loss: 1.4042 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 93/100 | Train Loss: 1.3214 | Val Loss: 1.4043 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 94/100 | Train Loss: 1.3352 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "Epoch 95/100 | Train Loss: 1.3157 | Val Loss: 1.4045 | Val Acc: 16.67% | Same accuracy streak: 1/10\n",
      "Epoch 96/100 | Train Loss: 1.3287 | Val Loss: 1.4046 | Val Acc: 16.67% | Same accuracy streak: 2/10\n",
      "Epoch 97/100 | Train Loss: 1.3308 | Val Loss: 1.4047 | Val Acc: 16.67% | Same accuracy streak: 3/10\n",
      "Epoch 98/100 | Train Loss: 1.3281 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 0/10\n",
      "Epoch 99/100 | Train Loss: 1.3201 | Val Loss: 1.4040 | Val Acc: 14.58% | Same accuracy streak: 1/10\n",
      "Epoch 100/100 | Train Loss: 1.3255 | Val Loss: 1.4042 | Val Acc: 16.67% | Same accuracy streak: 0/10\n",
      "\n",
      "Training complete. Best validation accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = SimpleHybridLTC(\n",
    "    input_dim=22,         # Number of EEG channels\n",
    "    cnn_dim=32,           # CNN output channels\n",
    "    ltc_hidden_dim=64,   # LTC hidden size\n",
    "    num_classes=4,        # Number of classes\n",
    "    num_ltc_layers=1,     # Number of stacked LTC layers \n",
    ").to(device)\n",
    "\n",
    "model_path = 'ltc_cnn_model.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded model from checkpoint.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "num_epochs = 100\n",
    "previous_val_acc = None\n",
    "same_acc_streak = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (data, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * correct / total\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(test_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if previous_val_acc is None or val_acc != previous_val_acc:\n",
    "        same_acc_streak = 0  # Reset counter if there's any change in accuracy\n",
    "    else:\n",
    "        same_acc_streak += 1  # Increment counter if accuracy is the same\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'ltc_cnn_model.pth')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | 'f'Same accuracy streak: {same_acc_streak}/10')\n",
    "\n",
    "    # Early stopping triggered after 10 consecutive epochs with no change in accuracy\n",
    "    if same_acc_streak >= 10:\n",
    "        print(f'\\nEarly stopping triggered after {epoch+1} epochs!')\n",
    "        break\n",
    "    \n",
    "    # Update previous_val_acc for the next iteration\n",
    "    previous_val_acc = val_acc\n",
    "\n",
    "# Load best model and final evaluation\n",
    "print(f'\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e75905-ceca-4974-9ab4-cd2ed27c5036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93045f5-ffaf-466c-99b8-102d3b0781d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
