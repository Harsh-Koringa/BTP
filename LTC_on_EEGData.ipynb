{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86515d-beac-4090-bd64-3e817f9e6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.signal import welch  # Import Welch's method for band power computation\n",
    "from scipy.stats import entropy  # Import entropy for spectral entropy calculation\n",
    "from scipy.stats import moment\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load EEG data\n",
    "data = np.load('A01T.npz')  \n",
    "signal = data['s']\n",
    "event_types = data['etyp'].T[0]\n",
    "event_positions = data['epos'].T[0]\n",
    "event_durations = data['edur'].T[0]\n",
    "\n",
    "# Extract signals & labels\n",
    "signals = []\n",
    "trial_types = []\n",
    "\n",
    "for i in range(7, len(event_positions) - 1):  \n",
    "    event_type = event_types[i]\n",
    "    next_event_type = event_types[i + 1]\n",
    "    \n",
    "    if event_type == 768:  # If it's a trial event\n",
    "        pos = event_positions[i]\n",
    "        dur = event_durations[i]\n",
    "        trial_signal = signal[pos:pos+dur, 0]  # Extract EEG channel 11\n",
    "        signals.append(trial_signal)\n",
    "        trial_types.append(next_event_type)  # Label is the next event type\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "def hjorth_parameters(signal):\n",
    "    \"\"\" Compute Hjorth mobility and complexity \"\"\"\n",
    "    first_deriv = np.diff(signal)\n",
    "    second_deriv = np.diff(first_deriv)\n",
    "\n",
    "    var_zero = np.var(signal) if np.var(signal) != 0 else 1e-6  # Avoid division by zero\n",
    "    var_diff = np.var(first_deriv) if np.var(first_deriv) != 0 else 1e-6\n",
    "    var_diff2 = np.var(second_deriv) if np.var(second_deriv) != 0 else 1e-6\n",
    "\n",
    "    mobility = np.sqrt(var_diff / var_zero)\n",
    "    complexity = np.sqrt(var_diff2 / var_diff)\n",
    "\n",
    "    return mobility, complexity\n",
    "\n",
    "def compute_band_power(trial, fs=256):\n",
    "    \"\"\" Compute power in Alpha, Beta, Theta, and Gamma bands using Welch's method \"\"\"\n",
    "    f, psd = welch(trial, fs=fs, nperseg=128)\n",
    "\n",
    "    alpha_power = np.sum(psd[(f >= 8) & (f <= 12)])  # Alpha: 8-12 Hz\n",
    "    beta_power = np.sum(psd[(f >= 12) & (f <= 30)])  # Beta: 12-30 Hz\n",
    "    theta_power = np.sum(psd[(f >= 4) & (f <= 8)])  # Theta: 4-8 Hz\n",
    "    gamma_power = np.sum(psd[(f >= 30) & (f <= 40)])  # Gamma: 30-40 Hz\n",
    "\n",
    "    return alpha_power, beta_power, theta_power, gamma_power\n",
    "\n",
    "def compute_fractal_dimension(signal, k_max=10):\n",
    "    \"\"\"\n",
    "    Compute the Higuchi Fractal Dimension (HFD) of a signal.\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    L = []\n",
    "    for k in range(1, k_max + 1):\n",
    "        Lk = 0\n",
    "        for m in range(k):\n",
    "            # Create subseries\n",
    "            subseries = signal[m::k]\n",
    "            # Compute the length of the subseries\n",
    "            Lkm = (np.sum(np.abs(np.diff(subseries))) * (N - 1)) / (len(subseries) * k)\n",
    "            Lk += Lkm\n",
    "        L.append(np.log(Lk / k))\n",
    "    # Fit a line to the log-log plot\n",
    "    hfd = np.polyfit(np.log(range(1, k_max + 1)), L, 1)[0]\n",
    "    return hfd\n",
    "\n",
    "def compute_sample_entropy(signal, m=2, r=0.2):\n",
    "    \"\"\"\n",
    "    Compute the Sample Entropy (SampEn) of a signal.\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    # Standardize the signal\n",
    "    signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "    # Compute the tolerance\n",
    "    r = r * np.std(signal)\n",
    "    # Split the signal into templates\n",
    "    def _phi(m):\n",
    "        templates = [signal[i:i + m] for i in range(N - m)]\n",
    "        B = 0\n",
    "        for i in range(len(templates)):\n",
    "            for j in range(len(templates)):\n",
    "                if i != j and np.max(np.abs(templates[i] - templates[j])) <= r:\n",
    "                    B += 1\n",
    "        return B / (N - m) / (N - m - 1)\n",
    "    # Compute SampEn\n",
    "    return -np.log(_phi(m + 1) / _phi(m))\n",
    "\n",
    "def extract_enhanced_features(trial_signals):\n",
    "    features = []\n",
    "    for trial in trial_signals:\n",
    "        # Time-domain features\n",
    "        mean_val = np.mean(trial)\n",
    "        std_val = np.std(trial)\n",
    "        skew_val = skew(trial)\n",
    "        kurt_val = kurtosis(trial)\n",
    "        max_val = np.max(trial)\n",
    "        min_val = np.min(trial)\n",
    "        peak_count, _ = find_peaks(trial)  # Number of peaks\n",
    "\n",
    "        # Frequency-domain features\n",
    "        alpha, beta, theta, gamma = compute_band_power(trial)\n",
    "        band_ratio_alpha_beta = alpha / beta if beta != 0 else 0\n",
    "\n",
    "        # Nonlinear features\n",
    "        mobility, complexity = hjorth_parameters(trial)\n",
    "        spectral_entropy = entropy(np.abs(trial) + 1e-6)\n",
    "\n",
    "        # Additional features\n",
    "        fractal_dimension = compute_fractal_dimension(trial)  # Implemented\n",
    "        sample_entropy = compute_sample_entropy(trial)  # Implemented\n",
    "\n",
    "        trial_features = [\n",
    "            mean_val, std_val, skew_val, kurt_val, max_val, min_val, \n",
    "            alpha, beta, theta, gamma, band_ratio_alpha_beta,\n",
    "            mobility, complexity, spectral_entropy,\n",
    "            len(peak_count), fractal_dimension, sample_entropy\n",
    "        ]\n",
    "        features.append(trial_features)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Compute features\n",
    "signals_features = extract_enhanced_features(signals)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "trial_types_encoded = label_encoder.fit_transform(np.array(trial_types))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(signals_features, trial_types_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for LTC (seq_len=17, feature_dim=1)\n",
    "X_train = X_train.reshape(X_train.shape[0], 17, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 17, 1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff05490-024e-4542-988d-d60c40d274b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.signal import welch  # Import Welch's method for band power computation\n",
    "from scipy.stats import entropy  # Import entropy for spectral entropy calculation\n",
    "\n",
    "\n",
    "\n",
    "# Load EEG data\n",
    "data = np.load('A01T.npz')  \n",
    "signal = data['s']\n",
    "event_types = data['etyp'].T[0]\n",
    "event_positions = data['epos'].T[0]\n",
    "event_durations = data['edur'].T[0]\n",
    "\n",
    "# Extract signals & labels\n",
    "signals = []\n",
    "trial_types = []\n",
    "\n",
    "for i in range(7, len(event_positions) - 1):  \n",
    "    event_type = event_types[i]\n",
    "    next_event_type = event_types[i + 1]\n",
    "    \n",
    "    if event_type == 768:  # If it's a trial event\n",
    "        pos = event_positions[i]\n",
    "        dur = event_durations[i]\n",
    "        trial_signal = signal[pos:pos+dur, 0:22]  # Extract all 22 EEG channels\n",
    "        signals.append(trial_signal)\n",
    "        trial_types.append(next_event_type)  # Label is the next event type\n",
    "\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "def hjorth_parameters(signal):\n",
    "    \"\"\" Compute Hjorth mobility and complexity \"\"\"\n",
    "    first_deriv = np.diff(signal)\n",
    "    second_deriv = np.diff(first_deriv)\n",
    "\n",
    "    var_zero = np.var(signal) if np.var(signal) != 0 else 1e-6  # Avoid division by zero\n",
    "    var_diff = np.var(first_deriv) if np.var(first_deriv) != 0 else 1e-6\n",
    "    var_diff2 = np.var(second_deriv) if np.var(second_deriv) != 0 else 1e-6\n",
    "\n",
    "    mobility = np.sqrt(var_diff / var_zero)\n",
    "    complexity = np.sqrt(var_diff2 / var_diff)\n",
    "\n",
    "    return mobility, complexity\n",
    "\n",
    "def compute_band_power(trial, fs=256):\n",
    "    \"\"\" Compute power in Alpha, Beta, Theta, and Gamma bands using Welch's method \"\"\"\n",
    "    f, psd = welch(trial, fs=fs, nperseg=128)\n",
    "\n",
    "    alpha_power = np.sum(psd[(f >= 8) & (f <= 12)])  # Alpha: 8-12 Hz\n",
    "    beta_power = np.sum(psd[(f >= 12) & (f <= 30)])  # Beta: 12-30 Hz\n",
    "    theta_power = np.sum(psd[(f >= 4) & (f <= 8)])  # Theta: 4-8 Hz\n",
    "    gamma_power = np.sum(psd[(f >= 30) & (f <= 40)])  # Gamma: 30-40 Hz\n",
    "\n",
    "    return alpha_power, beta_power, theta_power, gamma_power\n",
    "\n",
    "def extract_features(trial_signals):\n",
    "    features = []\n",
    "    \n",
    "    for trial in trial_signals:  # Loop over trials\n",
    "        trial_features = []\n",
    "        \n",
    "        for ch in range(22):  # Loop over 22 EEG channels\n",
    "            signal = trial[:, ch]  # Extract signal from this channel\n",
    "            \n",
    "            # Compute statistical features\n",
    "            mean_val = np.mean(signal)\n",
    "            std_val = np.std(signal)\n",
    "            skew_val = skew(signal)\n",
    "            kurt_val = kurtosis(signal)\n",
    "            max_val = np.max(signal)\n",
    "            min_val = np.min(signal)\n",
    "\n",
    "            # Compute spectral features\n",
    "            alpha, beta, theta, gamma = compute_band_power(signal)\n",
    "\n",
    "            # Compute Hjorth parameters\n",
    "            mobility, complexity = hjorth_parameters(signal)\n",
    "\n",
    "            # Compute Spectral Entropy\n",
    "            spectral_entropy = entropy(np.abs(signal) + 1e-6)  # Avoid log(0)\n",
    "\n",
    "            # Append all features for this channel\n",
    "            trial_features.extend([\n",
    "                mean_val, std_val, skew_val, kurt_val, max_val, min_val, \n",
    "                alpha, beta, theta, gamma, \n",
    "                mobility, complexity, spectral_entropy\n",
    "            ])\n",
    "        \n",
    "        features.append(trial_features)  # Add trial features to dataset\n",
    "    \n",
    "    return np.array(features)  # Final shape: (num_samples, 286)\n",
    "# Compute features\n",
    "signals_features = extract_features(signals)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "trial_types_encoded = label_encoder.fit_transform(np.array(trial_types))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(signals_features, trial_types_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for LTC (seq_len=286, feature_dim=1)\n",
    "X_train = X_train.reshape(X_train.shape[0], 286, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 286, 1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0e4dc-8740-411f-ad97-eaa3f85448e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LTC_Cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LTC_Cell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_xh = nn.Linear(input_dim, hidden_dim)  # Input transformation\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=False)  # Recurrent transformation\n",
    "        self.W_tau = nn.Linear(hidden_dim, hidden_dim)  # Time constant control\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)  # Normalize hidden state\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        tau = torch.sigmoid(self.W_tau(h)) + 0.1  # Ensure tau is positive\n",
    "        dh = -h / tau + torch.tanh(self.W_xh(x) + self.W_hh(h))\n",
    "        h_next = self.layer_norm(h + 0.1 * dh)  # Apply layer normalization\n",
    "        return h_next\n",
    "\n",
    "class Improved_LTC_RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.3):\n",
    "        super(Improved_LTC_RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.ltc_cell = LTC_Cell(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Regularization\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_dim)  # Initialize hidden state\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h = self.ltc_cell(x[:, t, :], h)  # Recurrent update\n",
    "\n",
    "        h = self.dropout(h)  # Apply dropout\n",
    "        output = self.fc(h)  # Output layer\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64ac7-e85d-45e2-b0d2-c551bb1fa0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 1  # Each feature is treated as an individual timestep\n",
    "hidden_dim = 64  # Increased hidden units for better learning\n",
    "output_dim = len(label_encoder.classes_)  # Number of classes\n",
    "dropout_rate = 0.4  # Increased dropout\n",
    "\n",
    "model = Improved_LTC_RNN(input_dim, hidden_dim, output_dim, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 1000\n",
    "best_loss = float('inf')\n",
    "patience = 15  # Stop training if no improvement for 15 epochs\n",
    "patience_counter = 0\n",
    "batch_size = 64\n",
    "\n",
    "train_losses, test_losses, accuracies = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test)\n",
    "        test_loss = criterion(test_output, y_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        predictions = torch.argmax(test_output, axis=1)\n",
    "        accuracy = (predictions == y_test).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Reduce learning rate if needed\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if test_loss.item() < best_loss:\n",
    "        best_loss = test_loss.item()\n",
    "        patience_counter = 0  # Reset counter if loss improves\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f}, Test Loss = {test_loss.item():.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9315e-ad38-43c6-aad7-e1d2ee176e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy Trend\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracies, label=\"Test Accuracy\", color='green')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate Predictions for Confusion Matrix\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test)\n",
    "    test_predictions = torch.argmax(test_output, axis=1)\n",
    "\n",
    "# Convert to Numpy for Visualization\n",
    "y_true = y_test.numpy()\n",
    "y_pred = test_predictions.numpy()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "#print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711732a-991f-4cf1-9eaa-728c6370af83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee36bd-c53c-4947-b949-91731b0c9691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
