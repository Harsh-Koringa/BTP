// EnhancedLTC Model Architecture
digraph {
	rankdir=TB
	node [fontname=Arial shape=box style=filled]
	size="7.5,10"
	input [label="Input Sequence
(batch, seq_len, 22)" fillcolor=lightblue]
	ltc1 [label="LTC Cell Layer 1
(22→64)" fillcolor=lightgreen]
	ltc2 [label="LTC Cell Layer 2
(64→64)" fillcolor=lightgreen]
	ltc_out [label="Hidden States Stack
(batch, seq_len, 64)" fillcolor=lightgreen]
	attention [label="Attention Layer
(learned weights)" fillcolor=plum]
	context [label="Context Vector
(batch, 64)" fillcolor=plum]
	fc1 [label="Linear
(64→32)" fillcolor=lightblue]
	relu [label=ReLU fillcolor=lightblue]
	dropout [label="Dropout(0.5)" fillcolor=lightblue]
	fc2 [label="Linear
(32→4)" fillcolor=lightblue]
	output [label="Output
(batch, 4)" fillcolor=lightblue]
	input -> ltc1
	ltc1 -> ltc2
	ltc2 -> ltc_out
	ltc_out -> attention
	attention -> context
	context -> fc1
	fc1 -> relu
	relu -> dropout
	dropout -> fc2
	fc2 -> output
	ltc1 -> ltc1 [label="τ" constraint=false]
	ltc2 -> ltc2 [label="τ" constraint=false]
	note1 [label="Adaptive time constants" color=gray fillcolor=white shape=note]
	note1 -> ltc1 [arrowhead=none color=gray style=dashed]
	note2 [label="Learned attention weights
over sequence" color=gray fillcolor=white shape=note]
	note2 -> attention [arrowhead=none color=gray style=dashed]
}
